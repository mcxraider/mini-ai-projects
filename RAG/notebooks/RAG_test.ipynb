{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Structured and multimodal RAG\n",
    "\n",
    "- We will use Unstructured to parse both text and tables from documents (PDFs).\n",
    "- We will use the multi-vector retriever to store raw tables, text along with table summaries better suited for retrieval.\n",
    "- We will use LCEL to implement the chains used.\n",
    "\n",
    "Notebook for reference: https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import yaml\n",
    "import requests\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "# Adobe PDF Services imports\n",
    "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
    "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
    "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
    "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
    "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
    "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
    "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
    "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
    "\n",
    "# Pinecone and Langchain imports\n",
    "from pinecone import Pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_groq import ChatGroq\n",
    "from groq import Groq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize clients\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Define model\n",
    "model = \"llama3-8b-8192\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ADOBE API to extract components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 14:23:09,972 - INFO - Started uploading asset\n",
      "2024-06-16 14:23:13,343 - INFO - Finished uploading asset\n",
      "2024-06-16 14:23:13,346 - INFO - Started submitting EXTRACT_PDF job\n",
      "2024-06-16 14:23:14,635 - INFO - Started getting job result\n",
      "2024-06-16 14:23:21,437 - INFO - Finished polling for status\n",
      "2024-06-16 14:23:21,439 - INFO - Finished getting job result\n",
      "2024-06-16 14:23:21,440 - INFO - Started getting content\n",
      "2024-06-16 14:23:21,767 - INFO - Finished getting content\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/HSI1000_1to9_unbocked.pdf'\n",
    "\n",
    "# Initialize the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractTextTableInfoFromPDF:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            file = open(file_path, 'rb')\n",
    "            input_stream = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Initial setup, create credentials instance\n",
    "            credentials = ServicePrincipalCredentials(\n",
    "                client_id=os.getenv('ADOBE_SERVICES_CLIENT_ID'),\n",
    "                client_secret=os.getenv('ADOBE_SERVICES_CLIENT_SECRET')\n",
    "            )\n",
    "\n",
    "            # Creates a PDF Services instance\n",
    "            pdf_services = PDFServices(credentials=credentials)\n",
    "\n",
    "            # Creates an asset(s) from source file(s) and upload\n",
    "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
    "\n",
    "            # Create parameters for the job\n",
    "            extract_pdf_params = ExtractPDFParams(\n",
    "                elements_to_extract=[ExtractElementType.TEXT, ExtractElementType.TABLES],\n",
    "            )\n",
    "\n",
    "            # Creates a new job instance\n",
    "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
    "\n",
    "            # Submit the job and gets the job result\n",
    "            location = pdf_services.submit(extract_pdf_job)\n",
    "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
    "\n",
    "            # Get content from the resulting asset(s)\n",
    "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
    "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
    "\n",
    "            \n",
    "            zip_bytes = io.BytesIO(stream_asset.get_input_stream())\n",
    "            with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "                # Extract all the contents into memory\n",
    "                self.extracted_data = {name: zip_ref.read(name) for name in zip_ref.namelist()}\n",
    "                \n",
    "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
    "            logging.exception(f'Exception encountered while executing operation: {e}')\n",
    "\n",
    "extractor = ExtractTextTableInfoFromPDF()\n",
    "extracted_data = extractor.extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_table_index_llama(table_str):\n",
    "    class Header(BaseModel):\n",
    "        index: int = Field(description=\"Header of the table, 0 for first row as the header, 1 for first column as the header\")\n",
    "        \n",
    "    parser = JsonOutputParser(pydantic_object=Header)\n",
    "\n",
    "    chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "    template = '''You will assist me in deciding, based on the first 2 entries of a table, whether the first row or the first colum should be the header. \n",
    "            You are to output an int, 0 or 1. Where 0 if the first row is header, and 1 if the first column is the header.\n",
    "            Follow the format instructions carefully.\n",
    "            Table:\n",
    "            {table}\n",
    "            \n",
    "            {format_instructions}\n",
    "            '''\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"table\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    chain = prompt | chat | parser\n",
    "    return chain.invoke({\"table\": table_str})\n",
    "\n",
    "def clean_values(x):\n",
    "    if isinstance(x, str):\n",
    "        return x.replace('_x000D_', '').strip()\n",
    "    return x\n",
    "\n",
    "def get_table_check_string(df):\n",
    "    table_str = \"\"\n",
    "    for i in range(2):\n",
    "        if i ==1:\n",
    "            table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\"  \n",
    "        else:\n",
    "            table_str += f\"Row {i}: {df.iloc[i].values.tolist()}\\n\"\n",
    "    return table_str\n",
    "\n",
    "def convert_table_to_str(df):\n",
    "    for index, row in df.iterrows():\n",
    "        row_str = \"\"\n",
    "        for col in df.columns:\n",
    "            sentences = re.split(r'(?<=\\.)\\s*', row[col])\n",
    "            row_sentence = \"\"\n",
    "            for i in range(len(sentences)):\n",
    "                row_sentence += sentences[i] + \"\\n\"\n",
    "            row_str += f\"{col}: {row_sentence}, \"\n",
    "        formatted = row_str[:-2]\n",
    "    return formatted\n",
    "    \n",
    "def get_table_meta(elements):\n",
    "    table_file_pages = {}\n",
    "    for el in elements:\n",
    "        # Using get to avoid KeyError and ensure 'filePaths' is not empty\n",
    "        file_paths = el.get('filePaths')\n",
    "        if file_paths:\n",
    "            page = el.get('Page', 'Unknown')  # Provide a default page number if missing\n",
    "            table_file_pages[file_paths[0]] = {\"Page\": page}\n",
    "    return table_file_pages\n",
    "\n",
    "# Process JSON data\n",
    "if 'structuredData.json' in extracted_data:\n",
    "    json_data = json.loads(extracted_data['structuredData.json'])\n",
    "    \n",
    "def get_table_pages_and_text_chunks(json_data):\n",
    "    if 'elements' not in json_data:\n",
    "        logging.error(\"Missing 'elements' key in json_data\")\n",
    "        raise ValueError(\"Missing 'elements' key in json_data\")\n",
    "\n",
    "    table_file_pages = {}\n",
    "    page_text = \"\"\n",
    "    start_page = 0\n",
    "    all_chunks = []\n",
    "    separator_list = [\"\\n\\n\", \"\\n\", \". \", \"!\", \"?\", \",\", \" \", \"\", \")\", \"(\"]\n",
    "    \n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=300,\n",
    "                chunk_overlap=50,\n",
    "                length_function=len,\n",
    "                separators=separator_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize text splitter: {e}\")\n",
    "        raise\n",
    "\n",
    "    list_label = \"\"\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            current_page = json_data['elements'][i]['Page']\n",
    "        except KeyError:\n",
    "            logging.warning(f\"Missing 'Page' key in element at index {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if current_page > start_page:\n",
    "                # Update the new page number\n",
    "                start_page = current_page               \n",
    "                # Generate the chunks for the previous page\n",
    "                separated_list = text_splitter.split_text(page_text)\n",
    "                for chunk in separated_list:\n",
    "                    if chunk not in [\". \", \".\"]:  # Simplified condition\n",
    "                        all_chunks.append({'ElementType': 'Text', 'Page': current_page, 'Text': chunk})\n",
    "                # Update the string of text \n",
    "                page_text = \"\"\n",
    "                list_label = \"\"\n",
    "            else:\n",
    "                if 'Text' in json_data['elements'][i]:  # Check if Text is not empty\n",
    "                    if json_data['elements'][i]['Path'].endswith(\"Lbl\") and not json_data['elements'][i]['Path'].startswith(\"//Document/Table\"):\n",
    "                        list_label = json_data['elements'][i]['Text']\n",
    "                    else:\n",
    "                        if list_label:\n",
    "                            page_text += list_label + json_data['elements'][i]['Text']\n",
    "                            list_label = \"\"  # Reset list label to empty string\n",
    "                        else:\n",
    "                            page_text += json_data['elements'][i]['Text'] + \"\\n\"\n",
    "        except KeyError as e:\n",
    "            logging.warning(f\"Key error in json_data['elements'][i] processing at index {i}: {e}\")\n",
    "    \n",
    "    # Process the last page of the text\n",
    "    if page_text:\n",
    "        separated_list = text_splitter.split_text(page_text)\n",
    "        for chunk in separated_list:\n",
    "            if chunk not in [\". \", \".\"]:\n",
    "                all_chunks.append({'ElementType': 'Text', 'Page': start_page + 1, 'Text': chunk})\n",
    " \n",
    "    # Obtaining the table metadata\n",
    "    for i in range(len(json_data['elements'])):\n",
    "        try:\n",
    "            file_paths = json_data['elements'][i].get('filePaths')\n",
    "            if file_paths:\n",
    "                page = json_data['elements'][i].get('Page', 'Unknown')\n",
    "                match = re.search(r'\\d+', file_paths[0])\n",
    "                table_index = match.group(0)\n",
    "                table_file_pages[int(table_index)] = {\"Page\": page}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file paths at index {i}: {e}\")\n",
    "    return table_file_pages, all_chunks\n",
    "\n",
    "table_file_pages, text_chunks = get_table_pages_and_text_chunks(json_data)\n",
    "\n",
    "#The literal extraction of the file itself\n",
    "excel_files = {k: v for k, v in extracted_data.items() if k.endswith('.xlsx')}\n",
    "\n",
    "table_dataframes = {}\n",
    "\n",
    "i=0\n",
    "for filename, content in excel_files.items():\n",
    "    excel_stream = BytesIO(content)\n",
    "    df = pd.read_excel(excel_stream, header=None)\n",
    "    df = df.applymap(clean_values)\n",
    "    df_str = get_table_check_string(df) \n",
    "    # dic = eval_table_index_llama(df_str)\n",
    "    # header_index = dic['index']\n",
    "    header_index = 1\n",
    "    \n",
    "    # ie header_index is non zero\n",
    "    if header_index:\n",
    "        df = pd.read_excel(excel_stream, header=None)\n",
    "        df = df.applymap(clean_values)\n",
    "        df = df.T\n",
    "        # Set the first row as the new header\n",
    "        new_header = df.iloc[0]  # Take the first row for the header\n",
    "        df = df[1:]  # Take the data less the header row\n",
    "        df.columns = new_header  # Set the header row as the df header\n",
    "        # Optionally, reset index if necessary\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    else:\n",
    "        df = pd.read_excel(excel_stream, header=0)\n",
    "    table_str = convert_table_to_str(df)\n",
    "    table_dataframes[i] = table_str\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Encoder()\n",
    "\n",
    "\t# Load embeddings. Need to change from ...co/models/ to ...co/pipeline/feature-extraction/...\n",
    "HF_API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-mpnet-base-v2\"\n",
    "headers = {\"Authorization\": f\"Bearer {hf_key}\"}\n",
    "\n",
    "def dense_embed(payload: str) -> str:\n",
    "        response = requests.post(HF_API_URL, headers=headers, json=payload)\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5c7e6f8fa84281a4b68628baa669fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def upsert_table_pinecone(table_dataframes, table_file_pages):\n",
    "    meta_table_batch = []\n",
    "    table_dfs = []\n",
    "    for table_index, table_str in table_dataframes.items():\n",
    "        dic = {}\n",
    "        dic['ElementType'] = 'Table'\n",
    "        dic['Page'] = table_file_pages[table_index]['Page']\n",
    "        dic['Table'] = table_dataframes[table_index]\n",
    "        table_dfs.append(dic)\n",
    "\n",
    "        meta_table_batch.append(f\"ElementType 'Table', Page {table_file_pages[table_index]['Page']}, {table_dataframes[table_index]}\")\n",
    "        \n",
    "    tables_df = pd.DataFrame(table_dfs)\n",
    "    df_dict = tables_df.to_dict(orient=\"records\") # Convert the batch to a list of dictionaries\n",
    "    \n",
    "    bm25.fit(meta_table_batch)\n",
    "\n",
    "    table_chunks = tables_df['Table'].tolist()\n",
    "        \n",
    "        # Encode combined metadata and text using BM25Encoder to create sparse embeddings\n",
    "    sparse_embeddings = bm25.encode_documents([combined for combined in meta_table_batch])\n",
    "\n",
    "        # Encode text using SentenceTransformer to create dense embeddings\n",
    "    dense_embeddings = dense_embed(table_chunks)\n",
    "    if not isinstance(dense_embeddings, list):\n",
    "        print(\"Embedding model not working properly\")\n",
    "        return None\n",
    "        # Generate a list of IDs for the current batch\n",
    "    ids = ['vec' +str(x) for x in range(len(meta_table_batch))]\n",
    "    pinecone_table_upserts = []\n",
    "        \n",
    "    for _id, sparse, dense, meta in zip(ids, sparse_embeddings, dense_embeddings, df_dict):\n",
    "            pinecone_table_upserts.append({\n",
    "                'id': _id,\n",
    "                'values': dense,\n",
    "                'sparse_values': sparse,\n",
    "                'metadata': meta\n",
    "            })\n",
    "        \n",
    "    index = pc.Index('hsi-notes')\n",
    "    chosen_namespace = 'page-1to9-with-metadata-tables'\n",
    "    # RUN ONLY WHEN WANT TO UPSERT NEW BATCH\n",
    "    if isinstance(dense_embeddings, list):\n",
    "        index.upsert(vectors = pinecone_table_upserts, namespace=chosen_namespace)\n",
    "    else:\n",
    "        print(\"Embedding model not connected properly. Dense embeddings not generated. \")\n",
    "        return\n",
    "    return pinecone_table_upserts\n",
    "table_data = upsert_table_pinecone(table_dataframes, table_file_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_table_chunks(query, top_k):\n",
    "    index = pc.Index('hsi-notes')\n",
    "    chosen_namespace = 'page-1to9-with-metadata-tables'\n",
    "    # Create dense vector of user query\n",
    "    dense_query = dense_embed(query)\n",
    "    sparse_query = bm25.encode_queries(query)\n",
    "    matches = index.query( \n",
    "        namespace=chosen_namespace,\n",
    "        filter={ \n",
    "                'ElementType': 'Table'\n",
    "            },\n",
    "        top_k=top_k, \n",
    "        vector=dense_query, \n",
    "        sparse_vector=sparse_query, \n",
    "        include_metadata=True\n",
    "        )\n",
    "    return matches\n",
    "\n",
    "def pretty_print_matches(result):\n",
    "    print(f\"Namespace searched: {result['namespace']}\\n\")\n",
    "    num_results = len(result['matches'])\n",
    "    print(f\"Top {num_results} relevant chunks found:\\n\")\n",
    "    for i in range(num_results):\n",
    "        print(f\"Found on page {int(result['matches'][i]['metadata']['Page'])}:\")\n",
    "        print(f\"{result['matches'][i]['metadata']['Table']}\")\n",
    "        print(f\"Dotproduct score: {result['matches'][i]['score']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def get_llm_context(query, top_k):\n",
    "    index_stats = pc.describe_index(os.environ['PINECONE_INDEX_NAME'])\n",
    "    if index_stats['status']['ready'] and index_stats['status']['state'] == \"Ready\":\n",
    "        relevant_matches = get_relevant_table_chunks(query, top_k) \n",
    "    pretty_print_matches(relevant_matches)\n",
    "    print(relevant_matches)       \n",
    "    # ideally its just to combine the first 2 matches. Or maybe to go by dotproduct score and difference \n",
    "    context = \"\"\n",
    "    for i in range(len(relevant_matches['matches'])):\n",
    "        context += f\"Page: {int(relevant_matches['matches'][i]['metadata']['Page'])} \" + relevant_matches['matches'][i]['metadata']['Table'] + \"\\n\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2479e4720945ce8692ddb8d04a3c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:08<00:24,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch starting with index 1 upserted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8039bf251e9842b58fdad0598fe74871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:13<00:13,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch starting with index 1 upserted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3e044c2aad40d2aab6b701ca7daa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:19<00:06,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch starting with index 1 upserted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb96299019c4053bac1dfbabc2c078f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:22<00:00,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch starting with index 1 upserted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def upsert_text_pinecone(text_documents):\n",
    "    # Convert text_documents to DataFrame\n",
    "    df = pd.DataFrame(text_documents)\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    # Loop through the DataFrame 'df' in batches of size 'batch_size'\n",
    "    for i in trange(0, len(df), batch_size):\n",
    "        i_end = min(i+batch_size, len(df)) # Determine the end index of the current batch\n",
    "        df_batch = df.iloc[i:i_end] # Extract the current batch from the DataFrame\n",
    "        df_dict = df_batch.to_dict(orient=\"records\") # Convert the batch to a list of dictionaries\n",
    "        \n",
    "        meta_text_batch = [\n",
    "            f\"ElementType 'Text', Page {row['Page']}: {row['Text']}\" for _, row in df_batch.iterrows()\n",
    "        ]\n",
    "        \n",
    "        bm25.fit(meta_text_batch)\n",
    "\n",
    "        text_chunks = df_batch['Text'].tolist()\n",
    "        \n",
    "        # Encode combined metadata and text using BM25Encoder to create sparse embeddings\n",
    "        sparse_embeddings = bm25.encode_documents([combined for combined in meta_text_batch])\n",
    "\n",
    "        # Encode text using SentenceTransformer to create dense embeddings\n",
    "        dense_embeddings = dense_embed(text_chunks)\n",
    "        \n",
    "        # Generate a list of IDs for the current batch\n",
    "        ids = ['vec' +str(x) for x in range(i, i_end)]\n",
    "        pinecone_batch_upserts = []\n",
    "        \n",
    "        for _id, sparse, dense, meta in zip(ids, sparse_embeddings, dense_embeddings, df_dict):\n",
    "            pinecone_batch_upserts.append({\n",
    "                'id': _id,\n",
    "                'values': dense,\n",
    "                'sparse_values': sparse,\n",
    "                'metadata': meta\n",
    "            })\n",
    "        \n",
    "        index = pc.Index('hsi-notes')\n",
    "        chosen_namespace = 'page-1to9-with-metadata'\n",
    "        # RUN ONLY WHEN WANT TO UPSERT NEW BATCH\n",
    "        if isinstance(dense_embeddings, list):\n",
    "            index.upsert(vectors = pinecone_batch_upserts, namespace=chosen_namespace)\n",
    "        else:\n",
    "            print(\"Embedding model not connected properly. Dense embeddings not generated. \")\n",
    "            return\n",
    "        print(f\"Batch starting with index {i} upserted\")\n",
    "    return\n",
    "\n",
    "upsert_text_pinecone(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the pinecone index to test the retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_chunks(query, top_k):\n",
    "    index = pc.Index('hsi-notes')\n",
    "    chosen_namespace = 'page-1to9-with-metadata'\n",
    "    # Create dense vector of user query\n",
    "    dense_query = dense_embed(query)\n",
    "    sparse_query = bm25.encode_queries(query)\n",
    "    matches = index.query( \n",
    "        namespace=chosen_namespace,\n",
    "        top_k=top_k, \n",
    "        vector=dense_query, \n",
    "        sparse_vector=sparse_query, \n",
    "        include_metadata=True\n",
    "        )\n",
    "    return matches\n",
    "\n",
    "def pretty_print_matches(result):\n",
    "    print(f\"Namespace searched: {result['namespace']}\\n\")\n",
    "    num_results = len(result['matches'])\n",
    "    print(f\"Top {num_results} relevant chunks found:\\n\")\n",
    "    for i in range(num_results):\n",
    "        print(f\"Found on page {int(result['matches'][i]['metadata']['Page'])}:\")\n",
    "        print(f\"{result['matches'][i]['metadata']['Text']}\")\n",
    "        print(f\"Dotproduct score: {result['matches'][i]['score']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def get_llm_context(query, top_k):\n",
    "    index_stats = pc.describe_index(os.environ['PINECONE_INDEX_NAME'])\n",
    "    if index_stats['status']['ready'] and index_stats['status']['state'] == \"Ready\":\n",
    "        relevant_matches = get_relevant_chunks(query, top_k) \n",
    "    pretty_print_matches(relevant_matches)       \n",
    "    # ideally its just to combine the first 2 matches. Or maybe to go by dotproduct score and difference \n",
    "    context = \"\"\n",
    "    for i in range(len(relevant_matches['matches'])):\n",
    "        context += f\"Page: {int(relevant_matches['matches'][i]['metadata']['Page'])} \" + relevant_matches['matches'][i]['metadata']['Text'] + \"\\n\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 13:52:44,944 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the notes, Dr. Semmelweis' friend, Professor Kolletschka, died after being accidentally pricked by a scalpel being used by a student doctor while assisting in an autopsy. This incident occurred in 1847.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def llama_chat(user_question, k):\n",
    "    context = get_llm_context(user_question, k)\n",
    "    context = '''Then a rather unfortunate incident occurred in 1847 in an anatomical pathology lab. Dr \n",
    "                Semmelweis’ friend, who he greatly admired, died after being accidentally pricked by a \n",
    "                scalpel being used by a student doctor while he was assisting in performing an autopsy. \n",
    "                Professor  Kolletschka  (Figure  3)  suffered  identical  signs  and  symptoms  as  the  mothers \n",
    "                who died of childbed fever. Dr Semmelweis wrote about the incident'''\n",
    "    chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "    system = '''\n",
    "            You are a science professor in a university. \n",
    "            Given the user's question and relevant sections from a set of school notes about scientific methodology and the history of science.\n",
    "            You will also answer the question by including direct quotes from the notes, \\\n",
    "            along with the page number where the answer or answers can be found.\n",
    "            '''\n",
    "    human = \"{text}\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\", system\n",
    "            ),\n",
    "            (\n",
    "                \"human\", human\n",
    "                )\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt | chat\n",
    "    return chain.invoke({\"text\": f\"User Question: \" + user_question + \"\\n\\nRelevant section in textbook:\\n\\n\" + context})\n",
    "\n",
    "answer = llama_chat(\"How did Dr Semmelweis' friend die?\", 5)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_element_paths(json_data):\n",
    "    for i, el in enumerate(json_data['elements']):\n",
    "        print(f\"{i}: {el['Path']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
