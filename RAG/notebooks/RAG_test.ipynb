{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Structured and multimodal RAG\n",
    "- We will use Unstructured to parse both text and tables from documents (PDFs).\n",
    "- We will use the multi-vector retriever to store raw tables, text along with table summaries better suited for retrieval.\n",
    "- We will use LCEL to implement the chains used.\n",
    "\n",
    "Notebook for reference: https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "import os\n",
    "import pinecone\n",
    "import requests\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load API Keys\n",
    "from unstructured.staging.base import elements_to_json, elements_from_json\n",
    "from unstructured.staging.base import convert_to_dict\n",
    "from unstructured.staging.base import convert_to_csv\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "import yaml\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#Need to import groq from langchain\n",
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Can try paddle OCR instead of tesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "groq_client = Groq(api_key = groq_api_key)\n",
    "model = \"llama3-8b-8192\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading \n",
    "- Using partitionpdf, which segments a pdf document by using a layout model.\n",
    "- This layout model makes it possible to extract elements, such as tables, from PDFs.\n",
    "- We will also use unstructured chunking\n",
    "  - Tries to identify document sections\n",
    "  - builds text blocks that maintain sections while also honoring user-defined chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from unstructured website and stack overflow \n",
    "path_to_hsi = \"../data/HSI1000_1to9.pdf\"\n",
    "raw_pdf_elements = partition_pdf(\"../data/HSI1000_1to9.pdf\", \n",
    "                        strategy=\"hi_res\", \n",
    "                        hi_res_model_name=\"yolox\",\n",
    "                        infer_table_structure=True,\n",
    "                        max_characters=2000,\n",
    "                        new_after_n_chars=1000,\n",
    "                        combine_text_under_n_chars=1000\n",
    "                        )\n",
    "\n",
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts\n",
    "\n",
    "# Save output to json file (Future use mongodb maybe)\n",
    "convert_to_dict(raw_pdf_elements)\n",
    "\n",
    "element_output_file = \"../data/element_entities.json\"\n",
    "elements_to_json(raw_pdf_elements, filename=element_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before filtering: 130\n",
      "length after filtering: 109\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/element_entities.json\", \"r\", encoding='utf-8') as fin:\n",
    "    read_elements = json.load(fin)\n",
    "print(f\"length before filtering: {len(read_elements)}\")\n",
    "\n",
    "unwanted_types = ['Footer', 'Image', 'FigureCaption', 'UncategorizedText']\n",
    "filtered_el = []\n",
    "for el in read_elements:\n",
    "    if el['type'] in unwanted_types:\n",
    "        continue\n",
    "    else:\n",
    "        filtered_el.append(el)\n",
    "print(f\"length after filtering: {len(filtered_el)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Title',\n",
       " 'element_id': 'cc9971d7967ab7ce6a3ac73cc065832e',\n",
       " 'metadata': {'coordinates': {'points': [[195.6, 158.1],\n",
       "    [195.6, 187.6],\n",
       "    [313.3, 187.6],\n",
       "    [313.3, 158.1]],\n",
       "   'system': 'PixelSpace',\n",
       "   'layout_width': 1653,\n",
       "   'layout_height': 2339},\n",
       "  'filename': 'HSI1000_1to9.pdf',\n",
       "  'file_directory': '../data',\n",
       "  'last_modified': '2024-06-12T13:15:53',\n",
       "  'filetype': 'application/pdf',\n",
       "  'page_number': 1},\n",
       " 'text': 'Lecture 1'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_el[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "table_elements =  [el for el in filtered_el if el['type'] == 'Table']\n",
    "print(len(table_elements))\n",
    "text_elements =  [el for el in filtered_el if el['type'] != 'Table']\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Title',\n",
       " 'element_id': 'cc9971d7967ab7ce6a3ac73cc065832e',\n",
       " 'metadata': {'coordinates': {'points': [[195.6, 158.1],\n",
       "    [195.6, 187.6],\n",
       "    [313.3, 187.6],\n",
       "    [313.3, 158.1]],\n",
       "   'system': 'PixelSpace',\n",
       "   'layout_width': 1653,\n",
       "   'layout_height': 2339},\n",
       "  'filename': 'HSI1000_1to9.pdf',\n",
       "  'file_directory': '../data',\n",
       "  'last_modified': '2024-06-12T13:15:53',\n",
       "  'filetype': 'application/pdf',\n",
       "  'page_number': 1},\n",
       " 'text': 'Lecture 1'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_elements[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Element Type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m row \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPage Number\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mpage_number \n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mElement Type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTable\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      8\u001b[0m     row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mtext_as_html\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Element Type'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=[]\n",
    "for c in raw_pdf_elements: \n",
    "    row = {}\n",
    "    row['Page Number'] = c.metadata.page_number \n",
    "    if row['Element Type'] == 'Table':\n",
    "        row['text'] = c.metadata.text_as_html\n",
    "    else:\n",
    "        row['text'] = c.text \n",
    "    data.append(row)\n",
    "  \n",
    "df = pd.DataFrame(data)\n",
    "df['combined'] = df.apply(lambda x: f\"Page Number: {x['Page Number']}\\n\\n{x['text']}\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Element Type: Title\\nFilename: HSI1000_1to9.pd...</td>\n",
       "      <td>{'Element Type': 'Title', 'Filename': 'HSI1000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Element Type: NarrativeText\\nFilename: HSI1000...</td>\n",
       "      <td>{'Element Type': 'NarrativeText', 'Filename': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Element Type: Title\\nFilename: HSI1000_1to9.pd...</td>\n",
       "      <td>{'Element Type': 'Title', 'Filename': 'HSI1000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Element Type: NarrativeText\\nFilename: HSI1000...</td>\n",
       "      <td>{'Element Type': 'NarrativeText', 'Filename': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Element Type: ListItem\\nFilename: HSI1000_1to9...</td>\n",
       "      <td>{'Element Type': 'ListItem', 'Filename': 'HSI1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Element Type: Title\\nFilename: HSI1000_1to9.pd...   \n",
       "1  Element Type: NarrativeText\\nFilename: HSI1000...   \n",
       "2  Element Type: Title\\nFilename: HSI1000_1to9.pd...   \n",
       "3  Element Type: NarrativeText\\nFilename: HSI1000...   \n",
       "4  Element Type: ListItem\\nFilename: HSI1000_1to9...   \n",
       "\n",
       "                                            metadata  \n",
       "0  {'Element Type': 'Title', 'Filename': 'HSI1000...  \n",
       "1  {'Element Type': 'NarrativeText', 'Filename': ...  \n",
       "2  {'Element Type': 'Title', 'Filename': 'HSI1000...  \n",
       "3  {'Element Type': 'NarrativeText', 'Filename': ...  \n",
       "4  {'Element Type': 'ListItem', 'Filename': 'HSI1...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "separator_ls = [\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=separator_ls\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "metadata = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    chunked_text = text_splitter.split_text(row['combined'])\n",
    "    for chunk in chunked_text:\n",
    "        chunks.append(chunk)\n",
    "        metadata.append({\n",
    "            'Page Number': row['Page Number']\n",
    "        })\n",
    "\n",
    "chunked_df = pd.DataFrame({\n",
    "    'text': chunks,\n",
    "    'metadata': metadata\n",
    "})\n",
    "\n",
    "chunked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Element Type: Title\\nFilename: HSI1000_1to9.pdf\\nDate Modified: 2024-06-12T13:15:53\\nFiletype: application/pdf\\nPage Number: 1\\n\\nLecture 1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_df.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before filtering: 130\n",
      "length after filtering: 109\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/element_entities.json\", \"r\", encoding='utf-8') as fin:\n",
    "    read_elements = json.load(fin)\n",
    "print(f\"length before filtering: {len(read_elements)}\")\n",
    "\n",
    "unwanted_types = ['Footer', 'Image', 'FigureCaption', 'UncategorizedText']\n",
    "filtered_el = []\n",
    "for el in read_elements:\n",
    "    if el['type'] in unwanted_types:\n",
    "        continue\n",
    "    else:\n",
    "        filtered_el.append(el)\n",
    "print(f\"length after filtering: {len(filtered_el)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "    \n",
    "table_elements =  [Element(type= 'Table', text=el['metadata']['text_as_html']) for el in filtered_el if el['type'] == 'Table']\n",
    "print(len(table_elements))\n",
    "text_elements =  [Element(type= el['type'], text=el['text']) for el in filtered_el if el['type'] != 'Table']\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import AsyncIterator, Iterator\n",
    "# from langchain_core.document_loaders import BaseLoader\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# # Custom class to load a string into a document from a string (which is what we have)\n",
    "# class CustomDocumentLoader(BaseLoader):\n",
    "#     \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
    "\n",
    "#     def __init__(self, file_path: str) -> None:\n",
    "#         \"\"\"Initialize the loader with a file path.\n",
    "\n",
    "#         Args:\n",
    "#             file_path: The path to the file to load.\n",
    "#         \"\"\"\n",
    "#         self.file_path = file_path\n",
    "\n",
    "#     def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments\n",
    "#         \"\"\"A lazy loader that reads a file line by line.\n",
    "\n",
    "#         When you're implementing lazy load methods, you should use a generator\n",
    "#         to yield documents one by one.\n",
    "#         \"\"\"\n",
    "#         with open(self.file_path, encoding=\"utf-8\") as f:\n",
    "#             line_number = 0\n",
    "#             for line in f:\n",
    "#                 yield Document(\n",
    "#                     page_content=line,\n",
    "#                     metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "#                 )\n",
    "#                 line_number += 1\n",
    "\n",
    "#     # alazy_load is OPTIONAL.\n",
    "#     # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!\n",
    "#     async def alazy_load(\n",
    "#         self,\n",
    "#     ) -> AsyncIterator[Document]:  # <-- Does not take any arguments\n",
    "#         \"\"\"An async lazy loader that reads a file line by line.\"\"\"\n",
    "#         # Requires aiofiles\n",
    "#         # Install with `pip install aiofiles`\n",
    "#         # https://github.com/Tinche/aiofiles\n",
    "#         import aiofiles\n",
    "\n",
    "#         async with aiofiles.open(self.file_path, encoding=\"utf-8\") as f:\n",
    "#             line_number = 0\n",
    "#             async for line in f:\n",
    "#                 yield Document(\n",
    "#                     page_content=line,\n",
    "#                     metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "#                 )\n",
    "#                 line_number += 1\n",
    "\n",
    "with open(\"../data/hsi_notes_1to9.txt\", \"w\", encoding=\"utf-8\") as fout: \n",
    "    document = \"\\n\".join([doc.text for doc in text_elements])\n",
    "    fout.write(document)\n",
    "    \n",
    "# loader = CustomDocumentLoader(\"../data/hsi_notes_1to9.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "# Implementing recursive text splitter:\n",
    "with open(\"../data/hsi_notes_1to9.txt\") as fin:\n",
    "    text_notes = fin.read()\n",
    "    \n",
    "seperator_ls = [\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=seperator_ls\n",
    ")\n",
    "\n",
    "text_chunks = text_splitter.create_documents([text_notes])\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of embeddings: 768\n",
      "First 20 embeddings: [-0.02262585423886776, -0.06885164231061935, -0.000889421091414988, -0.03406470641493797, -0.0012884179595857859, 0.001033041742630303, 0.0027252230793237686, 0.019959641620516777, 0.01079109963029623, -0.00538204051554203, 0.049704715609550476, 0.047944437712430954, 0.0277978777885437, -0.0077078077010810375, 0.04660750553011894, -0.10496175289154053, 0.03672698140144348, -0.007716397289186716, -0.02255360595881939, -0.02787116728723049]\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings. Need to change from ...co/models/ to ...co/pipeline/feature-extraction/...\n",
    "HF_API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-mpnet-base-v2\"\n",
    "headers = {\"Authorization\": f\"Bearer {hf_key}\"}\n",
    "\n",
    "def embed_documents(payload):\n",
    "\tresponse = requests.post(HF_API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "payload = [doc.page_content for doc in text_chunks]\n",
    "payload_embeddings = embed_documents(payload)\n",
    "print(f\"Dimension of embeddings: {len(payload_embeddings[0])}\\nFirst 20 embeddings: {payload_embeddings[0][:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Ignaz Semmelweis (Figure 1) was hired on a three-year contract into the Vienna General Hospital's maternity clinic from 1846 – 1849. At the time “childbed fever”, aka puerperal fever, was running rampant in hospitals all over Europe and the US\n"
     ]
    }
   ],
   "source": [
    "qn = \"How long was Dr ignaz's contract at the hospital?\"\n",
    "prompt_embeddings = embed_documents(qn) \n",
    "similarities = cosine_similarity([prompt_embeddings], payload_embeddings)[0] \n",
    "closest_similarity_index = np.argmax(similarities) \n",
    "most_relevant_chunk = text_chunks[closest_similarity_index].page_content\n",
    "print(f\"The most relevant chunk found:\\n{most_relevant_chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the relevant section in the textbook, Dr. Ignaz Semmelweis was hired on a three-year contract into the Vienna General Hospital's maternity clinic from 1846 to 1849.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def llama_chat(user_question, context):\n",
    "    chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "    system = '''\n",
    "            You are a science professor in a university. Given the user's question and relevant sections from a set of school notes,\\\n",
    "            answer the question by including direct quotes from the notes.\n",
    "            '''\n",
    "    human = \"{text}\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\", system\n",
    "            ),\n",
    "            (\n",
    "                \"human\", human\n",
    "                )\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt | chat\n",
    "    return chain.invoke({\"text\": f\"User Question: \" + user_question + \"\\n\\nRelevant section in textbook:\\n\\n\" + context})\n",
    "\n",
    "answer = llama_chat(qn, most_relevant_chunk)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Element(type='Table', text='<table><thead><th></th><th>Laptop doesn’t boot</th></thead><tr><td></td><td>Battery dead</td></tr><tr><td>the explanation |</td><td>Plug in external power</td></tr><tr><td>of test</td><td>Laptop seems to boot, so the battery must have been dead</td></tr></table>'),\n",
       " Element(type='Table', text='<table><tr><td rowspan=\"2\">Explanation Test the explanation |</td><td>Laptop monitor not working</td></tr><tr><td></td><td>Try connecting the external monitor with HDMI cable 1</td></tr><tr><td rowspan=\"2\">Result of test</td><td>Laptop seems to boot, but there’s nothing on the screen. (a) Either the graphics card or motherboard has issues,</td></tr><tr><td></td><td>or (b) Something was wrong with our test.</td></tr></table>'),\n",
       " Element(type='Table', text='<table><tr><td>Observation</td><td>Laptop seems boot, but there’s nothing on the screen</td></tr><tr><td>Explanation</td><td>Laptop monitor not working</td></tr><tr><td>Test the explanation</td><td>| Try connecting the external monitor with HDMI cable 2</td></tr><tr><td>Result of test</td><td>Laptop definitely boots and the external monitor shows the start screen.</td></tr></table>')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table section\n",
    "table_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerryyang/pythonenv/py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "\n",
    "pinecone_index_name = \"hsi-rag\"\n",
    "namespace = 'pages_1to9'\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "                                        documents=text_chunks,\n",
    "                                        embedding=embedding_function,\n",
    "                                        index_name=pinecone_index_name,\n",
    "                                        namespace = namespace)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Lecture 1\\nHSI1000\\n1 The Founding of Modern Science\\nIntended Learning Outcomes for Lecture 01 You should be able to do the following after this lecture.\\n(1) Describe what is science and explain the scientific method “in a nutshell”, illustrating your explanation with a straightforward example.', metadata={'text': 'Lecture 1\\nHSI1000\\n1 The Founding of Modern Science\\nIntended Learning Outcomes for Lecture 01 You should be able to do the following after this lecture.\\n(1) Describe what is science and explain the scientific method “in a nutshell”, illustrating your explanation with a straightforward example.'})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
