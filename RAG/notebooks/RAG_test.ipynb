{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Structured and multimodal RAG\n",
    "- We will use Unstructured to parse both text and tables from documents (PDFs).\n",
    "- We will use the multi-vector retriever to store raw tables, text along with table summaries better suited for retrieval.\n",
    "- We will use LCEL to implement the chains used.\n",
    "\n",
    "Notebook for reference: https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "import os\n",
    "import pinecone\n",
    "import requests\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load API Keys\n",
    "from unstructured.staging.base import elements_to_json, elements_from_json\n",
    "from unstructured.staging.base import convert_to_dict\n",
    "from unstructured.staging.base import convert_to_csv\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "import yaml\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#Need to import groq from langchain\n",
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Can try paddle OCR instead of tesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "groq_client = Groq(api_key = groq_api_key)\n",
    "model = \"llama3-8b-8192\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading \n",
    "- Using partitionpdf, which segments a pdf document by using a layout model.\n",
    "- This layout model makes it possible to extract elements, such as tables, from PDFs.\n",
    "- We will also use unstructured chunking\n",
    "  - Tries to identify document sections\n",
    "  - builds text blocks that maintain sections while also honoring user-defined chunk sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK:\n",
    "## params for ocr:high res model name, \n",
    "Parameters:\n",
    "strategy\n",
    "        The strategy to use for partitioning the PDF. Valid strategies are \"hi_res\",\n",
    "        \"ocr_only\", and \"fast\". When using the \"hi_res\" strategy, the function uses\n",
    "        a layout detection model to identify document elements. When using the\n",
    "        \"ocr_only\" strategy, partition_pdf simply extracts the text from the\n",
    "        document using OCR and processes it. If the \"fast\" strategy is used, the text\n",
    "        is extracted directly from the PDF. The default strategy `auto` will determine\n",
    "        when a page can be extracted using `fast` mode, otherwise it will fall back to `hi_res`.\n",
    "    infer_table_structure\n",
    "        Only applicable if `strategy=hi_res`.\n",
    "        If True, any Table elements that are extracted will also have a metadata field\n",
    "        named \"text_as_html\" where the table's text content is rendered into an html string.\n",
    "        I.e., rows and cells are preserved.\n",
    "        Whether True or False, the \"text\" field is always present in any Table element\n",
    "        and is the text content of the table (no structure).\n",
    "    languages\n",
    "        The languages present in the document, for use in partitioning and/or OCR. To use a language\n",
    "        with Tesseract, you'll first need to install the appropriate Tesseract language pack.\n",
    "    metadata_last_modified\n",
    "        The last modified date for the document.\n",
    "    hi_res_model_name\n",
    "        The layout detection model used when partitioning strategy is set to `hi_res`.\n",
    "    extract_images_in_pdf\n",
    "        Only applicable if `strategy=hi_res`.\n",
    "        If True, any detected images will be saved in the path specified by\n",
    "        'extract_image_block_output_dir' or stored as base64 encoded data within metadata fields.\n",
    "        Deprecation Note: This parameter is marked for deprecation. Future versions will use\n",
    "        'extract_image_block_types' for broader extraction capabilities.\n",
    "    extract_image_block_types\n",
    "        Only applicable if `strategy=hi_res`.\n",
    "        Images of the element type(s) specified in this list (e.g., [\"Image\", \"Table\"]) will be\n",
    "        saved in the path specified by 'extract_image_block_output_dir' or stored as base64\n",
    "        encoded data within metadata fields.\n",
    "    extract_image_block_to_payload\n",
    "        Only applicable if `strategy=hi_res`.\n",
    "        If True, images of the element type(s) defined in 'extract_image_block_types' will be\n",
    "        encoded as base64 data and stored in two metadata fields: 'image_base64' and\n",
    "        'image_mime_type'.\n",
    "        This parameter facilitates the inclusion of element data directly within the payload,\n",
    "        especially for web-based applications or APIs.\n",
    "    extract_image_block_output_dir\n",
    "        Only applicable if `strategy=hi_res` and `extract_image_block_to_payload=False`.\n",
    "        The filesystem path for saving images of the element type(s)\n",
    "        specified in 'extract_image_block_types'.\n",
    "    date_from_file_object\n",
    "        Applies only when providing file via `file` parameter. If this option is True, attempt\n",
    "        infer last_modified metadata from bytes, otherwise set it to None.\n",
    "    extract_forms\n",
    "        Whether the form extraction logic should be run\n",
    "        (results in adding FormKeysValues elements to output).\n",
    "    form_extraction_skip_tables\n",
    "        Whether the form extraction logic should ignore regions designated as Tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from unstructured website and stack overflow \n",
    "path_to_hsi = \"../data/HSI1000_1to9.pdf\"\n",
    "raw_pdf_elements = partition_pdf(\"../data/HSI1000_1to9.pdf\", \n",
    "                        strategy=\"hi_res\", \n",
    "                        hi_res_model_name=\"yolox\",\n",
    "                         infer_table_structure=True \n",
    "                        )\n",
    "\n",
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts\n",
    "\n",
    "# Save output to json file (Future use mongodb maybe)\n",
    "convert_to_dict(raw_pdf_elements)\n",
    "\n",
    "element_output_file = \"../data/element_entities.json\"\n",
    "elements_to_json(raw_pdf_elements, filename=element_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e03fc8939314b7eb2b817108cef5d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pinecone_text.sparse.bm25_encoder.BM25Encoder at 0x359ada020>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=[]\n",
    "for c in raw_pdf_elements: \n",
    "    row = {}\n",
    "    row['Element Type'] = type(c).__name__ \n",
    "    row['Filename'] = c.metadata.filename \n",
    "    row['Date Modified'] = c.metadata.last_modified \n",
    "    row['Filetype'] = c.metadata.filetype \n",
    "    row['Page Number'] = c.metadata.page_number \n",
    "    row['text'] = c.text \n",
    "    data.append(row)\n",
    "  \n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n",
    "\n",
    "# sparse vector\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "bm25 = BM25Encoder()\n",
    "bm25.fit(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take reference from unstructured blog on optimising for sparse and dense vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before filtering: 130\n",
      "length after filtering: 109\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/element_entities.json\", \"r\", encoding='utf-8') as fin:\n",
    "    read_elements = json.load(fin)\n",
    "print(f\"length before filtering: {len(read_elements)}\")\n",
    "\n",
    "unwanted_types = ['Footer', 'Image', 'FigureCaption', 'UncategorizedText']\n",
    "filtered_el = []\n",
    "for el in read_elements:\n",
    "    if el['type'] in unwanted_types:\n",
    "        continue\n",
    "    else:\n",
    "        filtered_el.append(el)\n",
    "print(f\"length after filtering: {len(filtered_el)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "    \n",
    "table_elements =  [Element(type= 'Table', text=el['metadata']['text_as_html']) for el in filtered_el if el['type'] == 'Table']\n",
    "print(len(table_elements))\n",
    "text_elements =  [Element(type= el['type'], text=el['text']) for el in filtered_el if el['type'] != 'Table']\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import AsyncIterator, Iterator\n",
    "# from langchain_core.document_loaders import BaseLoader\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# # Custom class to load a string into a document from a string (which is what we have)\n",
    "# class CustomDocumentLoader(BaseLoader):\n",
    "#     \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
    "\n",
    "#     def __init__(self, file_path: str) -> None:\n",
    "#         \"\"\"Initialize the loader with a file path.\n",
    "\n",
    "#         Args:\n",
    "#             file_path: The path to the file to load.\n",
    "#         \"\"\"\n",
    "#         self.file_path = file_path\n",
    "\n",
    "#     def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments\n",
    "#         \"\"\"A lazy loader that reads a file line by line.\n",
    "\n",
    "#         When you're implementing lazy load methods, you should use a generator\n",
    "#         to yield documents one by one.\n",
    "#         \"\"\"\n",
    "#         with open(self.file_path, encoding=\"utf-8\") as f:\n",
    "#             line_number = 0\n",
    "#             for line in f:\n",
    "#                 yield Document(\n",
    "#                     page_content=line,\n",
    "#                     metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "#                 )\n",
    "#                 line_number += 1\n",
    "\n",
    "#     # alazy_load is OPTIONAL.\n",
    "#     # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!\n",
    "#     async def alazy_load(\n",
    "#         self,\n",
    "#     ) -> AsyncIterator[Document]:  # <-- Does not take any arguments\n",
    "#         \"\"\"An async lazy loader that reads a file line by line.\"\"\"\n",
    "#         # Requires aiofiles\n",
    "#         # Install with `pip install aiofiles`\n",
    "#         # https://github.com/Tinche/aiofiles\n",
    "#         import aiofiles\n",
    "\n",
    "#         async with aiofiles.open(self.file_path, encoding=\"utf-8\") as f:\n",
    "#             line_number = 0\n",
    "#             async for line in f:\n",
    "#                 yield Document(\n",
    "#                     page_content=line,\n",
    "#                     metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "#                 )\n",
    "#                 line_number += 1\n",
    "\n",
    "with open(\"../data/hsi_notes_1to9.txt\", \"w\", encoding=\"utf-8\") as fout: \n",
    "    document = \"\\n\".join([doc.text for doc in text_elements])\n",
    "    fout.write(document)\n",
    "    \n",
    "# loader = CustomDocumentLoader(\"../data/hsi_notes_1to9.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "# Implementing recursive text splitter:\n",
    "with open(\"../data/hsi_notes_1to9.txt\") as fin:\n",
    "    text_notes = fin.read()\n",
    "    \n",
    "seperator_ls = [\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=seperator_ls\n",
    ")\n",
    "\n",
    "text_chunks = text_splitter.create_documents([text_notes])\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of embeddings: 768\n",
      "First 20 embeddings: [-0.02262585423886776, -0.06885164231061935, -0.000889421091414988, -0.03406470641493797, -0.0012884179595857859, 0.001033041742630303, 0.0027252230793237686, 0.019959641620516777, 0.01079109963029623, -0.00538204051554203, 0.049704715609550476, 0.047944437712430954, 0.0277978777885437, -0.0077078077010810375, 0.04660750553011894, -0.10496175289154053, 0.03672698140144348, -0.007716397289186716, -0.02255360595881939, -0.02787116728723049]\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings. Need to change from ...co/models/ to ...co/pipeline/feature-extraction/...\n",
    "HF_API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-mpnet-base-v2\"\n",
    "headers = {\"Authorization\": f\"Bearer {hf_key}\"}\n",
    "\n",
    "def embed_documents(payload):\n",
    "\tresponse = requests.post(HF_API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "payload = [doc.page_content for doc in text_chunks]\n",
    "payload_embeddings = embed_documents(payload)\n",
    "print(f\"Dimension of embeddings: {len(payload_embeddings[0])}\\nFirst 20 embeddings: {payload_embeddings[0][:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Ignaz Semmelweis (Figure 1) was hired on a three-year contract into the Vienna General Hospital's maternity clinic from 1846 – 1849. At the time “childbed fever”, aka puerperal fever, was running rampant in hospitals all over Europe and the US\n"
     ]
    }
   ],
   "source": [
    "qn = \"How long was Dr ignaz's contract at the hospital?\"\n",
    "prompt_embeddings = embed_documents(qn) \n",
    "similarities = cosine_similarity([prompt_embeddings], payload_embeddings)[0] \n",
    "closest_similarity_index = np.argmax(similarities) \n",
    "most_relevant_chunk = text_chunks[closest_similarity_index].page_content\n",
    "print(most_relevant_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the relevant section in the textbook, Dr. Ignaz Semmelweis was hired on a three-year contract into the Vienna General Hospital's maternity clinic from 1846 to 1849.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def llama_chat(user_question, context):\n",
    "    chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "    system = '''\n",
    "            You are a science professor in a university. Given the user's question and relevant sections from a set of school notes,\\\n",
    "            answer the question by including direct quotes from the notes.\n",
    "            '''\n",
    "    human = \"{text}\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\", system\n",
    "            ),\n",
    "            (\n",
    "                \"human\", human\n",
    "                )\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt | chat\n",
    "    return chain.invoke({\"text\": f\"User Question: \" + user_question + \"\\n\\nRelevant section in textbook:\\n\\n\" + context})\n",
    "\n",
    "answer = llama_chat(qn, most_relevant_chunk)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Element(type='Table', text='<table><thead><th></th><th>Laptop doesn’t boot</th></thead><tr><td></td><td>Battery dead</td></tr><tr><td>the explanation |</td><td>Plug in external power</td></tr><tr><td>of test</td><td>Laptop seems to boot, so the battery must have been dead</td></tr></table>'),\n",
       " Element(type='Table', text='<table><tr><td rowspan=\"2\">Explanation Test the explanation |</td><td>Laptop monitor not working</td></tr><tr><td></td><td>Try connecting the external monitor with HDMI cable 1</td></tr><tr><td rowspan=\"2\">Result of test</td><td>Laptop seems to boot, but there’s nothing on the screen. (a) Either the graphics card or motherboard has issues,</td></tr><tr><td></td><td>or (b) Something was wrong with our test.</td></tr></table>'),\n",
       " Element(type='Table', text='<table><tr><td>Observation</td><td>Laptop seems boot, but there’s nothing on the screen</td></tr><tr><td>Explanation</td><td>Laptop monitor not working</td></tr><tr><td>Test the explanation</td><td>| Try connecting the external monitor with HDMI cable 2</td></tr><tr><td>Result of test</td><td>Laptop definitely boots and the external monitor shows the start screen.</td></tr></table>')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table section\n",
    "table_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to initialise Pinecone Db\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone, ServerlessSpec, PineconeVectorStore\n",
    "import os\n",
    "import time\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index_name = \"hsi-rag\"\n",
    "\n",
    "# I want 2 namespaces, one for table embeddings and one for text embeddings\n",
    "text_namespace = \"text-embeddings\"\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_documents,\n",
    "    index_name=index_name,\n",
    "    embedding=text_embeddings, \n",
    "    namespace=text_namespace \n",
    ")\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
